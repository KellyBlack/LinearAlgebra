\documentclass[10pt]{book}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[mathscr]{euscript}
\usepackage{wasysym}	% cent symbol \cent
\usepackage[T1]{fontenc}
\usepackage{etoolbox}
\usepackage{qrcode}

\newtoggle{solutions}
\toggletrue{solutions}
\togglefalse{solutions}


%	%	%	%	%	%
%	Look & Layout	%
%	%	%	%	%	%

\usepackage[margin=1in]{geometry}

\usepackage{titlesec}
\titleformat{\chapter}[hang]{\LARGE\bfseries}{%
  \thechapter\hspace{1em}}{0pt}{\LARGE\bfseries} % Compact Chapter title
\titlespacing{\chapter}{0pt}{0pt}{0pt}	% Eliminate space before/after
% chapter
%\pagestyle{empty}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lfoot{Page \thepage}
\rfoot{~}
\cfoot{~}


\linespread{1.1}
\everymath{\displaystyle}
\setlength{\parindent}{0pt}

\usepackage{multicol}
\setlength{\columnsep}{30pt}
%\setlength\columnseprule{0.4pt}
\raggedcolumns

\usepackage[inline,shortlabels]{enumitem} % For inline lists
\usepackage{etoolbox}

\usepackage{tikz}
\usetikzlibrary{calc}	% For calculating coordinates e.g. ($ (P)+(\xlength,\height) $)
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{matrix}

\usepackage{pgfplots}
% As of 1.11, you may write tikz coordinates as (2,1) rather than having to type (axis cs:2,1)
\pgfplotsset{compat=1.11}

\newcommand{\Rand}{\pgfmathparse{random(7)}\pgfmathresult}

\newcommand{\boxcolor}{gray!30}
\usepackage{mdframed}
\newenvironment{boxme}{\begin{mdframed}[backgroundcolor=\boxcolor,linewidth=0pt,nobreak=true]}{\end{mdframed}}
\newenvironment{boxthm}{\begin{mdframed}[backgroundcolor=\boxcolor,nobreak=true]}{\end{mdframed}}
\newenvironment{boxdef}{\begin{mdframed}[backgroundcolor=\boxcolor,linewidth=0pt,nobreak=true]}{\end{mdframed}}


%	%	%	%	%	%	%	%
%	Exercise Environment	%
%	%	%	%	%	%	%	%

\usepackage{amsthm}

\newtheorem*{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\newtheorem{problem}{Problem}[section]
\renewcommand{\theexercise}{\arabic{exercise}}	% Shows Exercise 2
\renewcommand{\theproblem}{\arabic{problem}}	% Shows Problem 2 instead of Problem 1.1.2


%	%	%	%	%	%	%	%	%	%	%
%	Systems of Equations and Matrices	%
%	%	%	%	%	%	%	%	%	%	%

\usepackage{systeme}
%\syslineskipcoeff{1.2}\setlength{\tabskip}{3pt}
%\systeme{
%	2x  +   y  +  3z  =  10,
%	x  +   y  +   z  =   6,
%	x  +  3y  +  2z  =  13}

% Right align RHS of systeme
\makeatletter
\def\SYS@makesyspreamble@i#1{%
	\ifnum#1<\SYS@preamblenum
	\SYS@addtotok\SYS@systempreamble{\hfil$##$&\hfil$##$&}% 
	\expandafter\SYS@makesyspreamble@i\expandafter{\number\numexpr#1+\@ne\expandafter}%
	\else
	\SYS@addtotok\SYS@systempreamble{\hfil$##$&$##$&\hfil$##$\null}% 
	\ifSYS@extracol
	\SYS@addtotok\SYS@systempreamble{&\SYS@extracolstart##\SYS@extracolend\hfil\null}% 
	\fi
	\SYS@addtotok\SYS@systempreamble{\cr\SYS@strutup}% 
	\fi
}
\makeatother

% Remove brace from systeme
\syscodeextracol{\quad\hfill}{\hfill}
\sysdelim..

% Add alignment to matrices - default to right [r]
\makeatletter
\renewcommand*\env@matrix[1][r]{\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols #1}}
\makeatother

% Augmented matrix
\newenvironment{amatrix}[2]{%
	\left[\begin{array}{@{}*{#1}{r}|@{\hskip 1ex}@{}*{#2}{r}}
	}{%
\end{array}\right]
}

\usepackage[version=4]{mhchem}	% For chemical equations


%	%	%	%	% 	%
%	My Commands		%
%	%	%	%	%	%

% To save list number for resuming later
\newcounter{ListCounter}
%	\setcounter{ListCounter}{\value{enumi}}		% Save the item number
%	\setcounter{enumi}{\value{ListCounter}}		% Recall the item number

% Name Line
\newcommand{\name}[1][2.5in]{\vspace{-2.3em}\hfill Name: \underline{\hspace{#1}}}
% Group Names
\newcommand{\names}[1][6]{
	\vspace{-1em}
	\begin{multicols}{2}
		\foreach \xi in {1,...,#1}{
			Name: \underline{\hspace{2.25in}} \par\vspace{1em}
		}
	\end{multicols}
}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Poly}{\mathbb{P}}
\newcommand{\B}{\mathscr{B}}
\newcommand{\C}{\mathscr{C}}
\newcommand{\E}{\mathscr{E}}
\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\Nul}{Nul}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\Row}{Row}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}

\newcommand{\Axb}{A\vect{x}=\vect{b}}
\newcommand{\Axz}{A\vect{x}=\vect{0}}
\newcommand{\Ax}{A\vect{x}}
\newcommand{\Tx}{T(\vect{x})}
\newcommand{\ve}[1]{\vect{e}_{#1}}
\newcommand{\Te}[1]{T(\ve{#1})}
\newcommand{\Tmap}[2]{T:\R^{#1}\to\R^{#2}}
\newcommand{\vectset}[3][v]{\{\vect{#1}_{#2},\ldots,\vect{#1}_{#3}\}}
\newcommand{\vectsetvp}{\{\vect{v}_1,\ldots,\vect{v}_p\}}
\newcommand{\vectB}[1][x]{[\vect{#1}]_\B}
\newcommand{\vectC}[1][x]{[\vect{#1}]_\C}
\newcommand{\CoC}[2]{\underset{#2\leftarrow #1}{P}}

\newcommand{\Axlx}{A\vect{x}=\lambda\vect{x}}

\newcommand{\yhat}{\hat{\vect{y}}}
\newcommand{\xhat}{\hat{\vect{x}}}
\newcommand{\bhat}{\hat{\vect{b}}}
\newcommand{\Axbhat}{A\xhat=\bhat}
\newcommand{\NormEq}{A^TA\vect{x} = A^T\vect{b}}


% Special Matrices
\newcommand{\RotMat}[1][\theta]{\begin{bmatrix}\cos#1&-\sin#1\\ \sin#1&\cos#1\end{bmatrix}}
\newcommand{\ReflectMatHorz}{\begin{bmatrix}1&0\\0&-1\end{bmatrix}}
\newcommand{\ReflectMatVert}{\begin{bmatrix}-1&0\\0&1\end{bmatrix}}
\newcommand{\ReflectMatDiag}{\begin{bmatrix}0&1\\1&0\end{bmatrix}}
\newcommand{\ReflectMatRevDiag}{\begin{bmatrix}0&-1\\-1&0\end{bmatrix}}
\newcommand{\ReflectMatOrigin}{\begin{bmatrix}-1&0\\0&-1\end{bmatrix}}
\newcommand{\ShearMatHorz}[1][k]{\begin{bmatrix}1&#1\\0&1\end{bmatrix}}
\newcommand{\ShearMatVert}[1][k]{\begin{bmatrix}1&0\\#1&1\end{bmatrix}}
\newcommand{\ExpandMatHorz}[1][k]{\begin{bmatrix}#1&0\\0&1\end{bmatrix}}
\newcommand{\ExpandMatVert}[1][k]{\begin{bmatrix}1&0\\0&#1\end{bmatrix}}
\newcommand{\ExpandMatBoth}[1][k]{\begin{bmatrix}#1&0\\0&#1\end{bmatrix}}
\newcommand{\ProjMatHorz}{\begin{bmatrix}1&0\\0&0\end{bmatrix}}
\newcommand{\ProjMatVert}{\begin{bmatrix}0&0\\0&1\end{bmatrix}}

\newcommand{\Iarb}{\begin{bmatrix}1&0&0&\cdots&0\\0&1&0&\cdots&0\\0&0&1&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&1\end{bmatrix}}
\newcommand{\I}[1]{
	\ifnum#1=2 {\begin{bmatrix}1&0\\0&1\end{bmatrix}}
	\else\ifnum#1=3 {\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}}
	\else\ifnum#1=4 {\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix}}
	\else {I_n}
	\fi\fi\fi
}


\begin{document}

\thispagestyle{empty}

\title{Classroom Activities \\
  Introduction to Linear Algebra
  \iftoggle{solutions}{%
    \\\textit{Solution Manual}
  }
}
\author{University of Georgia\\Department of Mathematics}

\maketitle

%\thispagestyle{empty}
\noindent
Copyright (C) 2019-2020 Kelly Black, Toyin Alli, and Reeve Hunter
University of Georgia Department of Mathematics

\noindent
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License".

\vfill

\qrcode[height=1in,hyperlink,tight]{https://github.com/KellyBlack/LinearAlgebrax}


\tableofcontents
%\thispagestyle{empty}

\clearpage


% CH 1
\chapter[Linear Equations]{Linear Equations in Linear Algebra}
\setcounter{chapter}{1}
\chaptermark{Linear Equations}
\setcounter{section}{0}
%\pagestyle{empty}
%\thispagestyle{empty}

\include{linearEquations}



% CH 2
\chapter{Matrix Algebra}
\stepcounter{chapter}
\chaptermark{Matrix Algebra}
\setcounter{section}{0}
%\thispagestyle{empty}

\include{matrixAlgebra}

% CH 3
\chapter{Matrix Operations}
\stepcounter{chapter}
\chaptermark{Determinants}
\setcounter{section}{0}
%\thispagestyle{empty}

\include{matrixOperations}

% CH 4
\chapter{Vector Spaces}
\stepcounter{chapter}
\chaptermark{Vector Spaces}
\setcounter{section}{0}
%\thispagestyle{empty}




% CH 5
\chapter{Eigenvalues and Eigenvectors}
\stepcounter{chapter}
\chaptermark{Eigenvalues and Eigenvectors}
\setcounter{section}{0}
%\thispagestyle{empty}

\include{eigenVectors}

% CH 6
\chapter{Orthogonality and Least Squares}
\stepcounter{chapter}
\chaptermark{Orthogonality \& Least Squares}
\setcounter{section}{0}
%\thispagestyle{empty}

% SEC 6.1
\section[Inner Prod., Length, \& Orth.]{Inner Product, Length, \& Orthogonality}
\name[1.5in]


\begin{boxdef}
	The number $\vect{u}^T\vect{v}$ is called the \textbf{inner product} (or \textbf{dot product}) of $\vect{u}$ and $\vect{v}$. It is written as $\vect{u}\cdot\vect{v}$.
\end{boxdef}
% ADD properties of inner product??


\begin{exercise} % 6.1.2
	Let $\vect{w}=\begin{bmatrix}3\\-1\\-5\end{bmatrix}$ and $\vect{x}=\begin{bmatrix}6\\-2\\3\end{bmatrix}$. Compute the following.
	\begin{multicols}{2}
		\begin{enumerate}[(a)]
			\item $\vect{w}\cdot\vect{w}$
			\vspace{5em}
			\item $\vect{x}\cdot\vect{w}$
			\columnbreak
			\item $\frac{\vect{x}\cdot\vect{w}}{\vect{w}\cdot\vect{w}}$
		\end{enumerate}
	\end{multicols}
	\vspace{5em}
\end{exercise}


\begin{boxdef}
	The \textbf{length} (or \textbf{norm}) of a vector $\vect{v}$ in $\R^n$ is the nonnegative scalar $\|\vect{v}\|$ defined by
	$$ \|\vect{v}\| = \sqrt{\vect{v}\cdot\vect{v}} = \sqrt{v_1^2+v_2^2+\cdots+v_n^2}. $$
	A vector with length 1 is called a \textbf{unit vector}. To create a unit vector $\vect{u}$ from $\vect{v}$, compute $\vect{u}=\frac{1}{\|\vect{v}\|}\vect{v}$. You can think of this as ``dividing $\vect{v}$'' by its length, $\|\vect{v}\|$. This is called \textbf{normalizing} $\vect{v}$. We say the new unit vector $\vect{u}$ is \textbf{in the same direction} as $\vect{v}$.
\end{boxdef}


\begin{exercise} % 6.1.9,11
	For each problem below, find a unit vector in the direction of the given vector. \\
	Hint: You may wish to scale the vector by a positive constant before normalizing. This will not affect the final answer, but it can simplify calculations.
	\begin{multicols}{2}
		\begin{enumerate}[(a)]
			\item $\begin{bmatrix}-30\\40\end{bmatrix}$
			\columnbreak
			\item $\begin{bmatrix}7/4\\1/2\\-1/2\end{bmatrix}$
		\end{enumerate}
	\end{multicols}
\end{exercise}
\vfill


\newpage


\begin{boxdef}
	For $\vect{u}$ and $\vect{v}$ in $\R^n$, the \textbf{distance between $\boldsymbol{\vect{u}}$ and $\boldsymbol{\vect{v}}$}, written $\dist(\vect{u},\vect{v})$, is the length of the vector $(\vect{u}-\vect{v})$. That is,
	\vspace{-1ex}
	$$ \dist(\vect{u},\vect{v}) = \| \vect{u}-\vect{v} \|. $$
\end{boxdef}
\vspace{-1em}
\begin{boxdef}
	Two vectors $\vect{u}$ and $\vect{v}$ are \textbf{orthogonal} if $\vect{u}\cdot\vect{v}=0$. The zero vector, $\vect{0}$, is orthogonal to every vector.
\end{boxdef}


\begin{exercise} % 6.1.16
	The notion of orthogonality is a generalization of perpendicular lines. The vectors $\vect{u}$ and $\vect{v}$ are orthogonal if the distance between $\vect{u}$ and $\vect{v}$ is the same as the distance between $\vect{u}$ and $-\vect{v}$ (see the figure). Given $\vect{u}=\begin{bmatrix}12\\3\\-5\end{bmatrix}$ and $\vect{v}=\begin{bmatrix}2\\-3\\3\end{bmatrix}$, answer the following.

	\begin{enumerate}[(a)]
		\item Determine if $\vect{u}$ and $\vect{v}$ are orthogonal by comparing $\|\vect{u}-\vect{v}\|$ and $\|\vect{u}+\vect{v}\|$.
		
		\hfill
		\begin{tikzpicture}[scale=1]
		% Set u and v
		\pgfmathsetmacro{\ux}{4}
		\pgfmathsetmacro{\uy}{2}
		\pgfmathsetmacro{\vx}{-1}
		\pgfmathsetmacro{\vy}{2}
		% Begin Axis
		\begin{axis}[axis lines=none,
		axis x line=center, axis y line=middle, 
		xmin=-2.5, xmax=5.5,
		ymin=-3.5, ymax=3.5,
		scale only axis, axis equal, height=2in,
		grid=major, grid style={line width=.5pt, draw=gray!50, dashed}]
		% Plot vectors
		\fill[black] (0,0) circle (2pt) node[below left, fill=white, rounded corners=0.2cm] {$\vect{0}$};
		\fill[black] (\ux,\uy) circle (2pt) node[above right, fill=white, rounded corners=0.2cm] {$\vect{u}$};
		\fill[black] (\vx,\vy) circle (2pt) node[above left, fill=white, rounded corners=0.2cm] {$\vect{v}$};
		\fill[black] (-\vx,-\vy) circle (2pt) node[below right, fill=white, rounded corners=0.2cm] {$-\vect{v}$};
		\draw[<->,line width=1pt] (\vx,\vy) -- (-\vx,-\vy);
		\draw[->,line width=1pt] (0,0) -- (\ux,\uy);
		\draw[dashed,line width=1pt] (\vx,\vy) -- (\ux,\uy) node[midway, above] {$\|\vect{u}-\vect{v}\|$};
		\draw[dashed,line width=1pt] (-\vx,-\vy) -- (\ux,\uy) node[midway, below right] {$\|\vect{u}+\vect{v}\|$};
		\end{axis}
		\end{tikzpicture}
		\vspace{3em}
		
		\item Determine if $\vect{u}$ and $\vect{v}$ are orthogonal by computing $\vect{u}\cdot\vect{v}$.
		\vspace{5em}
	\end{enumerate}
\end{exercise}


\begin{boxdef}
	If $\vect{z}$ is orthogonal to every vector in a subspace $W$, then we say $\vect{z}$ is \textbf{orthogonal} to $W$. The set of all $\vect{z}$ orthogonal to $W$ is called the \textbf{orthogonal complement} of $W$, denoted $W^\perp$. A vector $\vect{z}$ is in $W^\perp$ if, and only if, $\vect{z}$ is orthogonal to every vector in a spanning set for $W$.
\end{boxdef}

\begin{exercise} % 6.1.Custom
	Suppose $W$ is spanned by the set $\left\{ \begin{bmatrix}-4\\1\\-2\\6\end{bmatrix},\begin{bmatrix}2\\-5\\-1\\4\end{bmatrix} \right\}$. Is $\vect{u}=\begin{bmatrix}3\\2\\-5\\0\end{bmatrix}$ in $W^\perp$?
\end{exercise}
\vfill


\newpage


% SEC 6.2
\section{Orthogonal Sets}
\name

\begin{boxme}
	A set of vectors $\vectsetvp$ in $\R^n$ is said to be an \textbf{orthogonal set} if each pair of distinct vectors from the set is orthogonal, that is, if $\vect{v}_i\cdot\vect{v}_j=0$ whenever $i\neq j$.
\end{boxme}


\begin{exercise} % 6.2.4 (scaled)
	Determine if the set $\{\vect{u}_1,\vect{u}_2,\vect{u}_3\}$ is orthogonal.
	$$ \vect{u}_1 = \begin{bmatrix}2\\-2\\1\\2\end{bmatrix}, \quad
	\vect{u}_2 = \begin{bmatrix}-1\\4\\-4\\7\end{bmatrix}, \quad
	\vect{u}_3 = \begin{bmatrix}4\\7\\6\\0\end{bmatrix} $$
\end{exercise}
\vfill


\begin{boxthm}
	\textbf{Theorem 6.4.} \\
	If $S=\vectset[u]{1}{p}$ is an orthogonal set of nonzero vectors in $\R^n$, then $S$ is linearly independent and hence is a basis for the subspace spanned by $S$.
\end{boxthm}
\vspace{-1em}
\begin{boxthm}
	\textbf{Theorem 6.5.} \\
	Let $\vectset[u]{1}{p}$ be an orthogonal basis for a subspace $W$ of $\R^n$. For each $\vect{y}$ in $W$, the weights in the linear combination
	\vspace{-1ex}
	$$ \vect{y} = c_1\vect{u}_1+\cdots+c_p\vect{u}_p $$
	are given by 
	\vspace{-1ex}
	$$ c_j = \frac{\vect{y}\cdot\vect{u}_j}{\vect{u}_j\cdot\vect{u}_j} \qquad
	(\text{for } j=1,\ldots,p). $$
\end{boxthm}


\begin{exercise} % 6.2.10 (Don't have to check orthogonality)
	The set $\{\vect{u}_1,\vect{u}_2,\vect{u}_3\}$ is orthogonal. By Theorem 6.4 (and Theorem 2.15 or 4.12, The Basis Theorem), the set is a basis for $\R^3$. Write $\vect{x}$ as a linear combination of the basis vectors: $\vect{x} = c_1\vect{u}_1 + c_2\vect{u}_2 + c_3\vect{u}_3$.
	$$ \vect{u}_1 = \begin{bmatrix}2\\-2\\0\end{bmatrix}, \quad
	\vect{u}_2 = \begin{bmatrix}3\\3\\-1\end{bmatrix}, \quad
	\vect{u}_3 = \begin{bmatrix}1\\1\\6\end{bmatrix}, \quad
	\vect{x} = \begin{bmatrix}4\\-3\\1\end{bmatrix} $$
\end{exercise}
\vfill


\newpage


\begin{boxdef}
	Suppose $\vect{u}$ and $\vect{y}$ are given. It will be useful to write $\vect{y}=\yhat+\vect{z}$ where $\yhat$ is some scalar multiple of $\vect{u}$ and $\vect{z}$ is orthogonal to $\vect{u}$. The vector $\yhat$ is called the \textbf{orthogonal projection of $\vect{y}$ onto $\vect{u}$}, and the vector $\vect{z}$ is called the \textbf{component of $\vect{y}$ orthogonal to $\vect{u}$}.
	\vspace{-1em}
	\begin{multicols}{2}
		\begin{center}
		\begin{tikzpicture}[scale=1]
		% Set u and v
		\pgfmathsetmacro{\ux}{5}
		\pgfmathsetmacro{\uy}{1.5}
		\pgfmathsetmacro{\yx}{3}
		\pgfmathsetmacro{\yy}{2.5}
		\pgfmathparse{\yx*\ux+\yy*\uy}
		\pgfmathsetmacro{\ydotu}{\pgfmathresult}
		\pgfmathparse{\ux*\ux+\uy*\uy}
		\pgfmathsetmacro{\udotu}{\pgfmathresult}
		\pgfmathparse{\ydotu/\udotu*\ux}
		\pgfmathsetmacro{\yhatx}{\pgfmathresult}
		\pgfmathparse{\ydotu/\udotu*\uy}
		\pgfmathsetmacro{\yhaty}{\pgfmathresult}
		\pgfmathparse{\yx-\yhatx}
		\pgfmathsetmacro{\zx}{\pgfmathresult}
		\pgfmathparse{\yy-\yhaty}
		\pgfmathsetmacro{\zy}{\pgfmathresult}
		% Begin Axis
		\begin{axis}[axis lines=none,
		axis x line=center, axis y line=middle,
		xmin=-.5, xmax=5.5,
		ymin=-3.5, ymax=6.5,
		%	xtick={-3,...,5}, ytick={-3,...,7},
		%	xticklabels={,,}, yticklabels={,,},
		scale only axis, axis equal, % height=2in,
		gray, grid=major, grid style={line width=.5pt, draw=gray!50, dashed}]
		% Set Coordinates
		\coordinate (O) at (0,0);
		\coordinate (U) at (\ux,\uy);
		\coordinate (Y) at (\yx,\yy);
		\coordinate (YHAT) at (\yhatx,\yhaty);
		\coordinate (Z) at (\zx,\zy);
		% Connect Vectors
		\draw[line width=1pt] (O) -- (Y);
		\draw[cyan, line width=1pt] (O) -- (Z);
		\draw[dashed, line width=1pt] (Z) -- (Y) -- (YHAT);
		% Plot u and Span{u}
		\addplot[-, cyan, line width=1pt, domain=-1.5:6.5]{(\uy/\ux)*x} node[above left] {$L$};
		% Plot vectors
		\fill[black] (O) circle (2pt) node[below] {$\vect{0}$};
		\fill[black] (U) circle (2pt) node[above] {$\vect{u}$};
		\fill[black] (Y) circle (2pt) node[above right] {$\vect{y}$};
		\fill[black] (YHAT) circle (2pt) node[below right] {$\yhat=\proj_L\vect{y}$};
		\fill[black] (Z) circle (2pt) node[above] {$\vect{z}=\vect{y}-\yhat$};
		% ADD RIGHT ANGLE
		\end{axis}
		\end{tikzpicture}
		\end{center}

		\columnbreak
		
		If we let $L=\Span\{\vect{u}\}$, then we write
		$$ \yhat = \proj_L\vect{y} = \frac{\vect{y}\cdot\vect{u}}{\vect{u}\cdot\vect{u}}\vect{u} $$
	\end{multicols}
\end{boxdef}


\begin{exercise} % 6.2.12
	\begin{enumerate}[(a)]
		\item Compute the orthogonal projection of $\vect{y}=\begin{bmatrix}-4\\3\end{bmatrix}$ onto the line through $\vect{u}=\begin{bmatrix}-1\\4\end{bmatrix}$ and the origin.
		\vfill
		
		\item Using part (a), how could you compute the shortest distance from $\vect{y}$ to $L=\Span{\vect{u}}$?
		\vspace{3em}
	\end{enumerate}
\end{exercise}


\begin{boxdef}
	A set $\vectsetvp$ is an \textbf{orthonormal} if it is an orthogonal set of unit vectors. If $W=\Span\vectsetvp$, then the set is an \textbf{orthonormal basis} for $W$.
\end{boxdef}

\begin{exercise} % 6.2.17
	Determine if the set of vectors is orthonormal. If the set is only orthogonal, normalize the vectors to produce an orthonormal set.
	$$ \vect{u}_1 = \begin{bmatrix}2/3\\1/3\\1/3\end{bmatrix}, \quad
	\vect{u}_2 = \begin{bmatrix}1/\sqrt{5}\\0\\-2/\sqrt{5}\end{bmatrix} $$
\end{exercise}
\vfill


\newpage


% SEC 6.3
\section{Orthogonal Projections}
\name

\begin{boxthm}
	\textbf{Theorem 6.8.}
	\textbf{The Orthogonal Decomposition Theorem} \\
	Let $W$ be a subspace of $\R^n$. Then each $\vect{y}$ in $\R^n$ can be written uniquely in the form
	$ \vect{y} = \yhat+\vect{z} $
	where $\yhat$ is in $W$ and $\vect{z}$ is in $W^\perp$. In fact, if $\vectset[u]{1}{p}$ is any orthogonal basis of $W$, then
	$$ \yhat = \frac{\vect{y}\cdot\vect{u}_1}{\vect{u}_1\cdot\vect{u}_1}\vect{u}_1 + \cdots + \frac{\vect{y}\cdot\vect{u}_p}{\vect{u}_p\cdot\vect{u}_p}\vect{u}_p \qquad \text{and} \qquad \vect{z}=\vect{y}-\yhat. $$
	The vector $\yhat$ is called the \textbf{orthogonal projection of $\boldsymbol{\vect{y}}$ onto $\boldsymbol{W}$}, and the vector $\vect{z}$ is called the \textbf{component of $\boldsymbol{\vect{y}}$ orthogonal to $\boldsymbol{W}$}.
\end{boxthm}


\begin{exercise} % 6.3.5
	Verify that $\{\vect{u}_1,\vect{u}_2\}$ is an orthogonal set, and then find $\yhat$, the orthogonal projection of $\vect{y}$ onto $\Span\{\vect{u}_1,\vect{u}_2\}$. Hint: If you do the problem correctly, $\yhat$ has all integer entries.
	$$ \vect{y} = \begin{bmatrix}-1\\3\\6\end{bmatrix}, \quad
	\vect{u}_1 = \begin{bmatrix}-5\\-1\\-2\end{bmatrix}, \quad
	\vect{u}_2 = \begin{bmatrix}1\\-1\\-2\end{bmatrix}$$
\end{exercise}
\vfill


\begin{exercise} % 6.3.1
	Write $\vect{x}$ as the sum of two vectors, one in $\Span\{\vect{u}_1,\vect{u}_2,\vect{u}_3\}$ and the other in $\Span\{\vect{u}_4\}$. You may assume $\{\vect{u}_1,\vect{u}_2,\vect{u}_3,\vect{u}_4\}$ is an orthogonal basis for $\R^4$.
	$$ \vect{u}_1 = \begin{bmatrix}0\\1\\-4\\-1\end{bmatrix}, \quad
	\vect{u}_2 = \begin{bmatrix}3\\5\\1\\1\end{bmatrix}, \quad
	\vect{u}_3 = \begin{bmatrix}1\\0\\1\\-4\end{bmatrix}, \quad
	\vect{u}_4 = \begin{bmatrix}5\\-3\\-1\\1\end{bmatrix}, \quad
	\vect{x} = \begin{bmatrix}10\\-8\\2\\0\end{bmatrix}$$
	Hint: You could compute the orthogonal projections of $\vect{x}$ onto $\Span\{\vect{u}_1,\vect{u}_2,\vect{u}_3\}$ and $\Span\{\vect{u}_4\}$, but there is a much quicker method using Theorem 6.8. If we let $\xhat$ be the orthogonal projection of $\vect{x}$ onto $W=\Span\{\vect{u}_4\}$. Then $\vect{z}=\vect{x}-\xhat$ is in $W^\perp=\Span\{\vect{u}_1,\vect{u}_2,\vect{u}_3\}$. So $\vect{x}=\xhat+\vect{z}$ will be the sum you want.
\end{exercise}
\vfill


\newpage


\begin{boxthm}
	\textbf{Theorem 6.9.}
	\textbf{The Best Approximation Theorem} \\
	Let $W$ be a subspace of $\R^n$, let $\vect{y}$ be any vector in $\R^n$, and let $\yhat$ be the orthogonal projection of $\vect{y}$ onto $W$. Then $\yhat$ is the closest point in $W$ to $\vect{y}$, in the sense that
	$$ \|\vect{y}-\yhat\| < \|\vect{y}-\vect{v}\| $$
	for all $\vect{v}$ in $W$ distinct from $\yhat$. The vector $\yhat$ is \textbf{the best approximation to $\boldsymbol{\vect{y}}$ by elements of $\boldsymbol{W}$}.
\end{boxthm}


\begin{exercise} % 6.3.12
	Find the closest point to $\vect{y}$ in the subspace $W$ spanned by $\vect{v}_1$ and $\vect{v}_2$. Assume $\vect{v}_1$ and $\vect{v}_2$ are orthogonal.
	$$ \vect{y} = \begin{bmatrix}3\\-1\\1\\13\end{bmatrix}, \quad
	\vect{v}_1 = \begin{bmatrix}1\\-2\\-1\\2\end{bmatrix}, \quad
	\vect{v}_2 = \begin{bmatrix}-4\\1\\0\\3\end{bmatrix} $$
\end{exercise}
\vfill


\begin{boxthm}
	\textbf{Theorem 6.10.} \\
	If $\vectset[u]{1}{p}$ is an orthonormal basis for a subspace $W$ of $\R^n$, then
	$$ \proj_W\vect{y} = (\vect{y}\cdot\vect{u}_1)\vect{u}_1 + (\vect{y}\cdot\vect{u}_2)\vect{u}_2 + \cdots + (\vect{y}\cdot\vect{u}_p)\vect{u}_p. $$
	If $U=\begin{bmatrix}\vect{u}_1&\vect{u}_2&\cdots&\vect{u}_p\end{bmatrix}$, then
	$ \proj_W\vect{y} = UU^T\vect{y}$ for all $\vect{y}$ in $\R^n$.
\end{boxthm}


\begin{exercise} % 6.3.17
	Let $\vect{y}=\begin{bmatrix}4\\8\\1\end{bmatrix}$, $\vect{u}_1=\begin{bmatrix}2/3\\1/3\\2/3\end{bmatrix}$, $\vect{u}_2=\begin{bmatrix}-2/3\\2/3\\1/3\end{bmatrix}$, and $W=\Span\{\vect{u}_1,\vect{u}_2\}$. You may assume $\{\vect{u}_1,\vect{u}_2\}$ is orthonormal.
	\begin{enumerate}[(a)]
		\item Let $U=\begin{bmatrix}\vect{u}_1&\vect{u}_2\end{bmatrix}$ and compute $UU^T$.
		
		$UU^T = \begin{bmatrix}2/3&-2/3\\1/3&2/3\\2/3&1/3\end{bmatrix}
		\begin{bmatrix}2/3&1/3&2/3\\-2/3&2/3&1/3\end{bmatrix}=$
		\vspace{2em}
		\item Compute $\proj_W\vect{y}=UU^T\vect{y}$.
		\vspace{1in}
	\end{enumerate}
\end{exercise}


\newpage


% SEC 6.4
\section{The Gram-Schmidt Process}
\name

\begin{boxthm}
	\textbf{Theorem 6.11.}
	\textbf{The Gram-Schmidt Process} \\
	Given a basis $\vectset[x]{1}{p}$ for a nonzero subspace $W$ of $\R^n$, define
	\vspace{-1ex}
	\begin{align*}
	\vect{v}_1 &= \vect{x}_1 \\
	\vect{v}_2 &= \vect{x}_2 - \frac{\vect{x}_2\cdot\vect{v}_1}{\vect{v}_1\cdot\vect{v}_1}\vect{v}_1 \\
	\vect{v}_3 &= \vect{x}_3 - \frac{\vect{x}_3\cdot\vect{v}_1}{\vect{v}_1\cdot\vect{v}_1}\vect{v}_1 - \frac{\vect{x}_3\cdot\vect{v}_2}{\vect{v}_2\cdot\vect{v}_2}\vect{v}_2 \\
	&\vdots \\
	\vect{v}_p &= \vect{x}_p - \frac{\vect{x}_p\cdot\vect{v}_1}{\vect{v}_1\cdot\vect{v}_1}\vect{v}_1 - \frac{\vect{x}_p\cdot\vect{v}_2}{\vect{v}_2\cdot\vect{v}_2}\vect{v}_2 - \ldots -  \frac{\vect{x}_p\cdot\vect{v}_{p-1}}{\vect{v}_{p-1}\cdot\vect{v}_{p-1}}\vect{v}_{p-1}.
	\end{align*}
	Then $\vectsetvp$ is an orthogonal basis for $W$, and $\Span\vectset{1}{k} = \Span\vectset[x]{1}{k}$ for $k\leq p$.
\end{boxthm}


\begin{exercise} % 6.4.3
	The set $\{\vect{x}_1,\vect{x}_2\}$ is a basis for a subspace $W$. Use the Gram-Schmidt process to produce an orthogonal basis for $W$. Hint: Scaling vectors before you begin may simplify calculations.
	
	\vspace{1em}
	$ \vect{x}_1 = \begin{bmatrix}2\\-5\\4\end{bmatrix}, \quad
	\vect{x}_2 = \begin{bmatrix}6\\-6\\3\end{bmatrix} $
\end{exercise}
\vfill


\begin{exercise} % 6.4.10 (altered)
	A matrix $A$ with linearly independent columns is given below. Find an orthogonal basis for the column space of $A$. Note that columns 1 and 2 are already orthogonal. Hint: Recall that one possible basis for $\Col A$ consists of the pivot columns of $A$.
	
	\vspace{1em}
	$ A = \begin{bmatrix}-1&1&3\\3&0&1\\2&1&3\\1&-1&-1\end{bmatrix} $
\end{exercise}
\vfill


\newpage


\begin{exercise} % 6.4.8
	The set $\{\vect{v}_1,\vect{v}_2\}$ is an orthogonal basis for a subspace $W$. Find an orthonormal basis for $W$. Hint: Since scaling vectors does not affect orthogonality, you may wish to scale $\vect{v}_1$ and $\vect{v}_2$ before normalizing.
	
	\vspace{1em}
	$ \vect{v}_1 = \begin{bmatrix}2\\-6\\4\end{bmatrix}, \quad
	\vect{v}_2 = \begin{bmatrix}-5\\-5\\-5\end{bmatrix} $
\end{exercise}
\vspace{1.75in}


\begin{boxthm}
	\textbf{Theorem 6.12.}
	\textbf{The $\boldsymbol{QR}$ Factorization} \\
	If $A$ is an $m\times n$ matrix with linearly independent columns, then $A$ can be factored as $A=QR$, where $Q$ is an $m\times n$ matrix whose columns form an orthonormal basis for $\Col A$ and $R$ is an $n\times n$ upper triangular invertible matrix with positive entries on its diagonal.
\end{boxthm}
\vspace{-1em}
\begin{boxme}
	To produce $Q$, orthogonalize and normalize the columns of $A$, i.e., apply the Gram-Schmidt process (with normalization) to the columns of $A$. To produce $R$, use the following:
	\vspace{-1em}
	\begin{align*}
	QR &= A \\
	Q^TQR &= Q^TA \\
	IR &= Q^TA &\text{(By Thm 6.6, since $Q$ has orthonormal columns, $Q^TQ=I$)}\\
	R &= Q^TA
	\end{align*}
\end{boxme}


\begin{exercise} % 6.4.14
	The columns of $Q$ were obtained by applying the Gram-Schmidt process (with normalization) to the columns of $A$. Find an upper triangular matrix $R$ such that $A=QR$.
	
	\vspace{1em}
	$ A= \begin{bmatrix}-2&-3\\5&7\\-2&-2\\-4&-1\end{bmatrix}, \quad
	Q= \begin{bmatrix}-2/7&-1/\sqrt{14}\\5/7&2/\sqrt{14}\\-2/7&0\\-4/7&3/\sqrt{14}\\\end{bmatrix}$
%	\begin{align*}
%	A &= \begin{bmatrix}-2&-3\\5&7\\-2&-2\\-4&-1\end{bmatrix} &
%	Q &= \begin{bmatrix}-2/7&-1/\sqrt{14}\\5/7&2/\sqrt{14}\\-2/7&0\\-4/7&3/\sqrt{14}\\\end{bmatrix}
%	\end{align*}
\end{exercise}
\vfill


\newpage


% SEC 6.5
\section{Least-Squares Problems}
\name

%\begin{boxdef}
%If $A$ is $m\times n$ and $\vect{b}$ is in $\R^m$, a \textbf{least-squares solution} of $\Axb$ is an $\xhat$ in $\R^n$ such that
%\vspace{-1ex}
%$$ \|\vect{b}-A\xhat\| \leq \|\vect{b}-A\vect{x}\| \qquad
%\text{for all $\vect{x}$ in $\R^n$.} $$
%\end{boxdef}

\begin{boxthm}
	\textbf{Theorem 6.13.} \\
	The set of least-squares solutions of $\Axb$ coincides with the nonempty set of solutions of the normal equations $\NormEq$.
\end{boxthm}


\begin{exercise} % 6.5.5
	Describe all the least-squares solutions of the equation $\Axb$.
	\begin{multicols}{2}
		$ A = \begin{bmatrix}1&1&0\\1&1&0\\1&0&1\\1&0&1\end{bmatrix}, 
		\vect{b} = \begin{bmatrix}2\\6\\5\\1\end{bmatrix} $
		\begin{enumerate}[(a)]
			\item Compute $A^TA$ and $A^T\vect{b}$. \\
			$ A^TA = \begin{bmatrix}1&1&1&1\\1&1&0&0\\0&0&1&1\end{bmatrix} \begin{bmatrix}1&1&0\\1&1&0\\1&0&1\\1&0&1\end{bmatrix} = $
			
			\vspace{2em}
			$ A^T\vect{b} = \begin{bmatrix}1&1&1&1\\1&1&0&0\\0&0&1&1\end{bmatrix} \begin{bmatrix}2\\6\\5\\1\end{bmatrix} = $
			\columnbreak
			\item Solve $\NormEq$. \\
			(You may use a calculator or computer)
		\end{enumerate}
	\end{multicols}
\end{exercise}
\vspace{2em}


\begin{boxthm}
	\textbf{Theorem 6.14.} \\
	Let $A$ be an $m\times n$ matrix. The following statements are logically equivalent:
	\vspace{-1ex}
	\begin{enumerate}[(a)]\itemsep=0em
		\item The equation $\Axb$ has a unique least-squares solution for each $\vect{b}$ in $\R^m$.
		\item The columns of $A$ are linearly independent.
		\item The matrix $A^TA$ is invertible.
	\end{enumerate}
	\vspace{-1ex}
	When these statements are true, the least-squares solution $\xhat$ is given by
	$ \xhat = \left(A^TA\right)^{-1}A^T\vect{b}. $
\end{boxthm}


\begin{exercise} % 6.5.1
	Find a least-squares solution of $\Axb$. Use Theorem 6.14 if applicable. \par
	$ A = \begin{bmatrix}-1&2\\2&-3\\-1&3\end{bmatrix}, 
	\vect{b} = \begin{bmatrix}16\\4\\8\end{bmatrix} $
	
	\begin{multicols}{2}
	\begin{align*}
	A^TA &= \begin{bmatrix}-1&2&-1\\2&-3&3\end{bmatrix}
	\begin{bmatrix}-1&2\\2&-3\\-1 &3\end{bmatrix} \\
	&= \begin{bmatrix}6&-11\\-11&22\end{bmatrix} \\[1em]
	A^T\vect{b} &= \begin{bmatrix}-1&2&-1\\2&-3&3\end{bmatrix} \begin{bmatrix}16\\4\\8\end{bmatrix} = \begin{bmatrix}-16\\44\end{bmatrix}
	\end{align*}
	
	\columnbreak
	\end{multicols}
\end{exercise}
\vfill


\newpage


\begin{exercise} % 6.5.10
	Let $ A = \begin{bmatrix}1&2\\-1&4\\1&2\end{bmatrix} $ and $ \vect{b} = \begin{bmatrix}2\\-1\\6\end{bmatrix} $.
	\begin{enumerate}[(a)]
		\item Find the orthogonal projection of $\vect{b}$ onto $\Col A$. Note that the columns of $A$ are linearly independent and orthogonal, so you already have an orthogonal basis for $\Col A$.
		\vspace{1.75in}
		\item Use your calculations in part (a) to find a least-squares solution of $\Axb$. \par Hint: Think of the entries of $\vect{x}$ as weights for a linear combination of the columns of $A$.
		\vspace{4em}
	\end{enumerate}
\end{exercise}


\begin{boxthm}
	\textbf{Theorem 6.15.} \\
	Given an $m\times n$ matrix $A$ with linearly independent columns, let $A=QR$ be a $QR$ factorization of $A$ as in Theorem 6.12. Then, for each $\vect{b}$ in $\R^m$, the equation $\Axb$ has a unique least-squares solution, given by
	\vspace{-1em}
	\begin{align*}
	\xhat &= R^{-1}Q^T\vect{b}. &
	(\text{Numerical Note: it is usually much quicker to solve } R\xhat = Q^T\vect{b}.)
	\end{align*}
\end{boxthm}


\begin{exercise} % 6.5.15
	Use the factorization $A=QR$ given below to find the least-squares solution of $\Axb$.
	\begin{align*}
	A &= \begin{bmatrix}2&3\\2&4\\1&1\end{bmatrix} =
	\begin{bmatrix}2/3&-1/3\\2/3&2/3\\1/3&-2/3\end{bmatrix}
	\begin{bmatrix}3&5\\0&1\end{bmatrix} &
	\vect{b} &= \begin{bmatrix}6\\3\\9\end{bmatrix}
	\end{align*}
	\begin{enumerate}[(a)]
		\item Compute $Q^T\vect{b}$. \par
		$ Q^T\vect{b}= \begin{bmatrix}2/3&2/3&1/3\\-1/3&2/3&-2/3\end{bmatrix}
		\begin{bmatrix}6\\3\\9\end{bmatrix} = $
		\vspace{2em}
		\item Solve $R\xhat=Q^T\vect{b}$ via back-substitution.
		\vspace{1in}
	\end{enumerate}
\end{exercise}


\newpage


% CH 7
\chapter{Symmetric Matrices and Quadratic Forms}
\stepcounter{chapter}
\chaptermark{Symmetric Matrices \& Quad. Forms}
\setcounter{section}{0}
%\thispagestyle{empty}

% SEC 7.1
\section[Diag. of Sym. Matrices]{Diagonalization of Symmetric Matrices}
\name[1.5in]

\begin{boxdef}
	A \textbf{symmetric matrix} is a matrix $A$ such that $A^T=A$.
\end{boxdef}
\vspace{-1em}
\begin{boxthm}
	\textbf{Theorem 7.1.} \\
	If $A$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.
\end{boxthm}


\begin{exercise} % 7.1.19 Custom
	\begin{enumerate}[(a)]
		\item Fill in the following matrix so that it is symmetric:
%		$$ A = \begin{bmatrix}2&-4&8\\-4&8&4\\8&4&2\end{bmatrix} $$	
		\begingroup
		\renewcommand{\arraystretch}{1.5}
		$$ A = \begin{bmatrix}2&&  &&8\\-4&&8&&4\\ && &&2\end{bmatrix} $$
		\endgroup
		\item The vectors $\vect{v}_1=\begin{bmatrix}-1\\2\\0\end{bmatrix}$, $\vect{v}_2=\begin{bmatrix}1\\0\\1\end{bmatrix}$, and $\vect{v}_3=\begin{bmatrix}2\\1\\-2\end{bmatrix}$ are eigenvectors of the symmetric matrix $A$ above. The eigenvalue for $\vect{v}_1$ and $\vect{v}_2$ is $\lambda=10$, and the eigenvalue for $\vect{v}_3$ is $\lambda=-8$. What does Theorem 7.1 tell you about $\vect{v}_1\cdot\vect{v}_2$, $\vect{v}_2\cdot\vect{v}_3$, and $\vect{v}_1\cdot\vect{v}_3$, if anything?
	\end{enumerate}
\end{exercise}
\vspace{2in}


\begin{boxdef}
	An \textbf{orthogonal matrix} (section 6.2) is a square matrix $A$ with orthonormal columns (so $A^{-1}=A^T$). \par
	A matrix $A$ is \textbf{orthogonally diagonalizable} if there are an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^{-1}=PDP^T$.
\end{boxdef}
\vspace{-1em}
\begin{boxthm}
	\textbf{Theorem 7.2.} \\
	An $n\times n$ matrix $A$ is orthogonally diagonalizable if, and only if, $A$ is a symmetric matrix.
\end{boxthm}


\begin{exercise} % 7.1.9 Custom
	\begin{enumerate}[(a)]
		\item Is $B$ an orthogonal matrix? Why or why not? \par
		$ B = \begin{bmatrix}2&-3\\3&2\end{bmatrix} $
		\item Give an example of a $3\times 3$ matrix that is orthogonally diagonalizable.
	\end{enumerate}
\end{exercise}
\vfill


\newpage


\begin{boxme}
	\textbf{Steps to Orthogonally Diagonalize an $\boldsymbol{n\times n}$ Symmetric Matrix}
	\begin{enumerate}[(i)]\itemsep=0em
		\item Find the eigenvalues of $A$ 
		\item Find $n$ linearly independent eigenvectors for $A$, orthogonalize the set (if necessary), and normalize
		\item Construct $P$ from the orthonormalized eigenvectors you obtain in step (ii)
		\item Construct $D$ by placing the corresponding eigenvectors along the diagonal of $D$
	\end{enumerate}
\end{boxme}


\begin{exercise} % 7.1.15 Custom
	A matrix and two eigenvectors are given below. Orthogonally diagonalize the matrix (you only need to determine $P$ and $D$). Steps (i) and (ii) should go quickly since you already have 2 eigenvectors. \par
	$ A = \begin{bmatrix}13&4\\4&7\end{bmatrix}, 
	\vect{v}_1 = \begin{bmatrix}-1\\2\end{bmatrix}, 
	\vect{v}_2 = \begin{bmatrix}2\\1\end{bmatrix} $
%	\begin{align*}
%	A &= \begin{bmatrix}13&4\\4&7\end{bmatrix} &
%	\vect{v}_1 &= \begin{bmatrix}-1\\2\end{bmatrix}, 
%	\vect{v}_2 = \begin{bmatrix}2\\1\end{bmatrix}
%	\end{align*}
\end{exercise}
\vfill


\begin{boxdef}
	The set of eigenvalues of a matrix $A$ is called the \textbf{spectrum} of $A$. A \textbf{spectral decomposition} for $A$ can be obtained from the columns of $P$ and eigenvalues from $D$ in the orthogonal diagonalization of $A$:
	\begin{align*}
	A &= PDP^T
	= \begin{bmatrix}\vect{u}_1&\cdots&\vect{u}_n\end{bmatrix}
	\begin{bmatrix}\lambda_1&&0\\&\ddots&\\0&&\lambda_n\end{bmatrix}
	\begin{bmatrix}[c]\vect{u}_1^T\\ \vdots\\ \vect{u}_n^T\end{bmatrix}
	= \begin{bmatrix}\lambda_1\vect{u}_1&\cdots&\lambda_n\vect{u}_n\end{bmatrix}
	\begin{bmatrix}[c]\vect{u}_1^T\\ \vdots\\ \vect{u}_n^T\end{bmatrix}
	\end{align*}
	This last expression can be rewritten as a spectral decomposition:
	$A = \lambda_1\vect{u}_1\vect{u}_1^T + \lambda_2\vect{u}_2\vect{u}_2^T + \cdots + \lambda_n\vect{u}_n\vect{u}_n^T.$ This is a sum of matrices each of which relies on just one column of $P$ and its corresponding eigenvalue.
\end{boxdef}


\begin{exercise} % 7.1.15 Custom
	Find a spectral decomposition for the matrix $A$ using the given orthogonal diagonalization, $A=PDP^T$.
	\begin{align*}
	A &= \begin{bmatrix}2&9\\9&2\end{bmatrix} &
	PDP^T &= 
	\begin{bmatrix}-1/\sqrt{2}&1/\sqrt{2}\\1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}
	\begin{bmatrix}-7&0\\0&11\end{bmatrix}
	\begin{bmatrix}-1/\sqrt{2}&1/\sqrt{2}\\1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}
	\end{align*}
	\begin{enumerate}[(a)]
		\item Compute $\vect{u}_1\vect{u}_1^T$ and $\vect{u}_2\vect{u}_2^T$, where $\vect{u}_1$ and $\vect{u}_2$ are the first and second columns of $P$, respectively.
		\vspace{1in}
		\item Write a spectral decomposition for $A$.
		\vspace{3em}
	\end{enumerate}
\end{exercise}


%\newpage
%
%
%% SEC 7.2
%\section{Quadratic Forms}
%\name
%
%\begin{exercise} % 7.2.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\begin{exercise} % 7.2.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\newpage
%
%
%\begin{exercise} % 7.2.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\begin{exercise} % 7.2.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\newpage
%
%
%\setcounter{section}{3}
%\section{The Singular Value Decomposition}
%\name
%
%\begin{exercise} % 7.4.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\begin{exercise} % 7.4.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\newpage
%
%
%\begin{exercise} % 7.4.
%	Text here.
%\end{exercise}
%\vfill
%
%
%\begin{exercise} % 7.4.
%	Text here.
%\end{exercise}
%\vfill


\end{document}



% Exercise Template:
\begin{exercise} % 7.X.
	Text here.
\end{exercise}
\vfill

% System template
$\systeme{
	x_1				-	3x_3 =  8,
	2x_1	+	2x_2	+	9x_3 =  7,
	x_2	+	5x_3 = -2}$
